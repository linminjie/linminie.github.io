"use strict";(globalThis.webpackChunkmimoe_devportal=globalThis.webpackChunkmimoe_devportal||[]).push([[9459],{1470(e,n,t){t.d(n,{A:()=>I});var s=t(6540),i=t(4164),r=t(7559),o=t(3104),l=t(6347),a=t(205),c=t(7485),d=t(1682),h=t(679);function m(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,s.useMemo)(()=>{const e=n??function(e){return m(e).map(({props:{value:e,label:n,attributes:t,default:s}})=>({value:e,label:n,attributes:t,default:s}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function x({queryString:e=!1,groupId:n}){const t=(0,l.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(i),(0,s.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(t.location.search);n.set(i,e),t.replace({...t.location,search:n.toString()})},[i,t])]}function j(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,r=u(e),[o,l]=(0,s.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:r})),[c,d]=x({queryString:t,groupId:i}),[m,j]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,i]=(0,h.Dv)(n);return[t,(0,s.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),g=(()=>{const e=c??m;return p({value:e,tabValues:r})?e:null})();(0,a.A)(()=>{g&&l(g)},[g]);return{selectedValue:o,selectValue:(0,s.useCallback)(e=>{if(!p({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),j(e)},[d,j,r]),tabValues:r}}var g=t(2303);const f="tabList__CuJ",v="tabItem_LNqP";var b=t(4848);function y({className:e,block:n,selectedValue:t,selectValue:s,tabValues:r}){const l=[],{blockElementScrollPositionUntilNextRender:a}=(0,o.a_)(),c=e=>{const n=e.currentTarget,i=l.indexOf(n),o=r[i].value;o!==t&&(a(n),s(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:s})=>(0,b.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:d,onClick:c,...s,className:(0,i.A)("tabs__item",v,s?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function A({lazy:e,children:n,selectedValue:t}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===t);return e?(0,s.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function k(e){const n=j(e);return(0,b.jsxs)("div",{className:(0,i.A)(r.G.tabs.container,"tabs-container",f),children:[(0,b.jsx)(y,{...n,...e}),(0,b.jsx)(A,{...n,...e})]})}function I(e){const n=(0,g.A)();return(0,b.jsx)(k,{...e,children:m(e.children)},String(n))}},4461(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"ai-foundation/inference/generative","title":"Generative AI","description":"Run AI inference with GGUF models using the OpenAI-compatible API","source":"@site/docs/ai-foundation/inference/generative.md","sourceDirName":"ai-foundation/inference","slug":"/ai-foundation/inference/generative","permalink":"/mimOE-devportal/docs/ai-foundation/inference/generative","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Generative AI","description":"Run AI inference with GGUF models using the OpenAI-compatible API"},"sidebar":"aiFoundationSidebar","previous":{"title":"Inference API","permalink":"/mimOE-devportal/docs/ai-foundation/inference/"},"next":{"title":"Predictive AI","permalink":"/mimOE-devportal/docs/ai-foundation/inference/predictive"}}');var i=t(4848),r=t(8453),o=t(1470),l=t(9365);const a={sidebar_position:1,title:"Generative AI",description:"Run AI inference with GGUF models using the OpenAI-compatible API"},c="Generative AI",d={},h=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Base URL",id:"base-url",level:2},{value:"Chat Completions",id:"chat-completions",level:2},{value:"Basic Chat Completion",id:"basic-chat-completion",level:3},{value:"System Messages",id:"system-messages",level:3},{value:"Multi-Turn Conversation",id:"multi-turn-conversation",level:3},{value:"Streaming Responses",id:"streaming-responses",level:3},{value:"Generation Parameters",id:"generation-parameters",level:3},{value:"Embeddings",id:"embeddings",level:2},{value:"Model Cache Management",id:"model-cache-management",level:2},{value:"List Loaded Models",id:"list-loaded-models",level:3},{value:"Pre-Load a Model",id:"pre-load-a-model",level:3},{value:"Unload a Model",id:"unload-a-model",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Handling Errors with OpenAI SDK",id:"handling-errors-with-openai-sdk",level:3},{value:"Performance Tips",id:"performance-tips",level:2},{value:"First Request Latency",id:"first-request-latency",level:3},{value:"Context Size",id:"context-size",level:3},{value:"Memory Management",id:"memory-management",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Slow First Request",id:"slow-first-request",level:3},{value:"Out of Memory",id:"out-of-memory",level:3},{value:"Token Limit Exceeded",id:"token-limit-exceeded",level:3},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"generative-ai",children:"Generative AI"})}),"\n",(0,i.jsx)(n.p,{children:"The Generative AI API provides an OpenAI-compatible interface for running LLM models on-device. This guide covers chat completions, embeddings, and model cache management."}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before making inference requests:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["mimOE runtime is running (",(0,i.jsx)(n.a,{href:"/mimOE-devportal/docs/ai-foundation/quick-start",children:"Quick Start"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["At least one model provisioned and ready in the Model Registry (",(0,i.jsx)(n.a,{href:"/mimOE-devportal/docs/ai-foundation/upload-model",children:"upload guide"}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"base-url",children:"Base URL"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"http://localhost:8083/mimik-ai/openai/v1\n"})}),"\n",(0,i.jsx)(n.p,{children:"All endpoints require authentication:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Authorization: Bearer 1234\n"})}),"\n",(0,i.jsx)(n.h2,{id:"chat-completions",children:"Chat Completions"}),"\n",(0,i.jsx)(n.p,{children:"Generate text responses using LLM models. The API follows the OpenAI chat completions format."}),"\n",(0,i.jsx)(n.h3,{id:"basic-chat-completion",children:"Basic Chat Completion"}),"\n","\n",(0,i.jsxs)(o.A,{children:[(0,i.jsx)(l.A,{value:"curl",label:"cURL",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "user", "content": "Complete this sentence: AI is like a"}\n    ]\n  }\'\n'})})}),(0,i.jsx)(l.A,{value:"js",label:"JavaScript",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\n\nconst client = new OpenAI({\n  baseURL: 'http://localhost:8083/mimik-ai/openai/v1',\n  apiKey: '1234'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'smollm2-360m',\n  messages: [\n    { role: 'user', content: 'Complete this sentence: AI is like a' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n"})})}),(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8083/mimik-ai/openai/v1",\n    api_key="1234"\n)\n\nresponse = client.chat.completions.create(\n    model="smollm2-360m",\n    messages=[\n        {"role": "user", "content": "Complete this sentence: AI is like a"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n'})})})]}),"\n",(0,i.jsx)(n.p,{children:"Response:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-abc123",\n  "object": "chat.completion",\n  "created": 1702742400,\n  "model": "smollm2-360m",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Running AI locally offers privacy (your data stays on-device), lower latency (no network round-trips), offline capability, and reduced cloud costs."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 8,\n    "completion_tokens": 28,\n    "total_tokens": 36\n  }\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"system-messages",children:"System Messages"}),"\n",(0,i.jsx)(n.p,{children:"Guide the model's behavior with a system message:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "system", "content": "You are a helpful coding assistant. Provide concise, accurate code examples."},\n      {"role": "user", "content": "Write a Python function to reverse a string"}\n    ]\n  }\'\n'})}),"\n",(0,i.jsx)(n.h3,{id:"multi-turn-conversation",children:"Multi-Turn Conversation"}),"\n",(0,i.jsx)(n.p,{children:"Include conversation history in the messages array:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "user", "content": "What is the capital of France?"},\n      {"role": "assistant", "content": "The capital of France is Paris."},\n      {"role": "user", "content": "What is its population?"}\n    ]\n  }\'\n'})}),"\n",(0,i.jsx)(n.admonition,{title:"Conversation Management",type:"tip",children:(0,i.jsx)(n.p,{children:"The API is stateless. You must include the full conversation history in each request. The model doesn't remember previous requests."})}),"\n",(0,i.jsx)(n.h3,{id:"streaming-responses",children:"Streaming Responses"}),"\n",(0,i.jsx)(n.p,{children:"Get responses token-by-token for real-time UX:"}),"\n",(0,i.jsxs)(o.A,{children:[(0,i.jsx)(l.A,{value:"js",label:"JavaScript",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\n\nconst client = new OpenAI({\n  baseURL: 'http://localhost:8083/mimik-ai/openai/v1',\n  apiKey: '1234'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'smollm2-360m',\n  messages: [{ role: 'user', content: 'Complete this sentence: Programming is like a' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n"})})}),(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8083/mimik-ai/openai/v1",\n    api_key="1234"\n)\n\nstream = client.chat.completions.create(\n    model="smollm2-360m",\n    messages=[{"role": "user", "content": "Complete this sentence: Programming is like a"}],\n    stream=True\n)\n\nfor chunk in stream:\n    content = chunk.choices[0].delta.content\n    if content:\n        print(content, end="", flush=True)\n'})})})]}),"\n",(0,i.jsx)(n.h3,{id:"generation-parameters",children:"Generation Parameters"}),"\n",(0,i.jsx)(n.p,{children:"Control the output with generation parameters:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [{"role": "user", "content": "Tell me a creative story"}],\n    "max_tokens": 200,\n    "temperature": 0.8,\n    "top_p": 0.95\n  }\'\n'})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Default"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"max_tokens"})}),(0,i.jsx)(n.td,{children:"integer"}),(0,i.jsx)(n.td,{children:"model limit"}),(0,i.jsx)(n.td,{children:"Maximum tokens to generate"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"temperature"})}),(0,i.jsx)(n.td,{children:"float (0-2)"}),(0,i.jsx)(n.td,{children:"1.0"}),(0,i.jsx)(n.td,{children:"Randomness (lower = more focused)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"top_p"})}),(0,i.jsx)(n.td,{children:"float (0-1)"}),(0,i.jsx)(n.td,{children:"1.0"}),(0,i.jsx)(n.td,{children:"Nucleus sampling threshold"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Temperature Guidelines:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"0.0-0.3"}),": Factual, deterministic responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"0.4-0.7"}),": Balanced creativity and coherence"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"0.8-1.5"}),": Creative, diverse outputs"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"embeddings",children:"Embeddings"}),"\n",(0,i.jsxs)(n.p,{children:["Generate vector embeddings from text using embedding models (",(0,i.jsx)(n.code,{children:"kind: embed"}),")."]}),"\n",(0,i.jsxs)(o.A,{children:[(0,i.jsx)(l.A,{value:"curl",label:"cURL",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/embeddings" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "nomic-embed-text",\n    "input": "The quick brown fox jumps over the lazy dog."\n  }\'\n'})})}),(0,i.jsx)(l.A,{value:"js",label:"JavaScript",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\n\nconst client = new OpenAI({\n  baseURL: 'http://localhost:8083/mimik-ai/openai/v1',\n  apiKey: '1234'\n});\n\n// Single text\nconst response = await client.embeddings.create({\n  model: 'nomic-embed-text',\n  input: 'The quick brown fox jumps over the lazy dog.'\n});\nconsole.log(response.data[0].embedding.slice(0, 5));  // First 5 dimensions\n\n// Batch embeddings\nconst batchResponse = await client.embeddings.create({\n  model: 'nomic-embed-text',\n  input: [\n    'First document to embed',\n    'Second document to embed',\n    'Third document to embed'\n  ]\n});\nbatchResponse.data.forEach(item => {\n  console.log(`Document ${item.index}: ${item.embedding.length} dimensions`);\n});\n"})})}),(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8083/mimik-ai/openai/v1",\n    api_key="1234"\n)\n\n# Single text\nresponse = client.embeddings.create(\n    model="nomic-embed-text",\n    input="The quick brown fox jumps over the lazy dog."\n)\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n\n# Batch embeddings\nresponse = client.embeddings.create(\n    model="nomic-embed-text",\n    input=[\n        "First document to embed",\n        "Second document to embed",\n        "Third document to embed"\n    ]\n)\nfor item in response.data:\n    print(f"Document {item.index}: {len(item.embedding)} dimensions")\n'})})})]}),"\n",(0,i.jsx)(n.h2,{id:"model-cache-management",children:"Model Cache Management"}),"\n",(0,i.jsx)(n.p,{children:"The Inference API maintains a runtime cache of loaded models. Models are automatically loaded on first inference, or you can pre-load them."}),"\n",(0,i.jsx)(n.h3,{id:"list-loaded-models",children:"List Loaded Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl "http://localhost:8083/mimik-ai/openai/v1/models" \\\n  -H "Authorization: Bearer 1234"\n'})}),"\n",(0,i.jsx)(n.p,{children:"Response:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [\n    {\n      "id": "smollm2-360m",\n      "object": "model",\n      "created": 1729591200,\n      "owned_by": "local"\n    }\n  ]\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"pre-load-a-model",children:"Pre-Load a Model"}),"\n",(0,i.jsx)(n.p,{children:"Load a model into cache before first inference:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/models" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{"model": "smollm2-360m"}\'\n'})}),"\n",(0,i.jsx)(n.p,{children:"This is useful for warming up the cache to avoid first-inference latency."}),"\n",(0,i.jsx)(n.h3,{id:"unload-a-model",children:"Unload a Model"}),"\n",(0,i.jsx)(n.p,{children:"Free memory by unloading a model from cache:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X DELETE "http://localhost:8083/mimik-ai/openai/v1/models/smollm2-360m" \\\n  -H "Authorization: Bearer 1234"\n'})}),"\n",(0,i.jsx)(n.p,{children:"The model file remains in the Model Registry and can be reloaded later."}),"\n",(0,i.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsx)(n.p,{children:"Common error responses:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Status"}),(0,i.jsx)(n.th,{children:"Cause"}),(0,i.jsx)(n.th,{children:"Solution"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"400"}),(0,i.jsx)(n.td,{children:"Model not ready"}),(0,i.jsx)(n.td,{children:"Complete model provisioning in Model Registry"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"401"}),(0,i.jsx)(n.td,{children:"Invalid API key"}),(0,i.jsx)(n.td,{children:"Check Authorization header"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"404"}),(0,i.jsx)(n.td,{children:"Model not found"}),(0,i.jsx)(n.td,{children:"Provision model in Model Registry first"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Error Response Format:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "error": {\n    "code": 404,\n    "message": "Model \'unknown-model\' not found in store"\n  }\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"handling-errors-with-openai-sdk",children:"Handling Errors with OpenAI SDK"}),"\n",(0,i.jsxs)(o.A,{children:[(0,i.jsx)(l.A,{value:"js",label:"JavaScript",default:!0,children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import OpenAI from 'openai';\n\nconst client = new OpenAI({\n  baseURL: 'http://localhost:8083/mimik-ai/openai/v1',\n  apiKey: '1234'\n});\n\ntry {\n  const response = await client.chat.completions.create({\n    model: 'smollm2-360m',\n    messages: [{ role: 'user', content: 'Hello!' }]\n  });\n  console.log(response.choices[0].message.content);\n} catch (error) {\n  if (error instanceof OpenAI.APIConnectionError) {\n    console.error('Could not connect to mimOE. Is the runtime running?');\n  } else if (error instanceof OpenAI.APIError) {\n    console.error(`API error: ${error.message}`);\n  }\n}\n"})})}),(0,i.jsx)(l.A,{value:"python",label:"Python",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI, APIError, APIConnectionError\n\nclient = OpenAI(\n    base_url="http://localhost:8083/mimik-ai/openai/v1",\n    api_key="1234"\n)\n\ntry:\n    response = client.chat.completions.create(\n        model="smollm2-360m",\n        messages=[{"role": "user", "content": "Hello!"}]\n    )\n    print(response.choices[0].message.content)\nexcept APIConnectionError:\n    print("Could not connect to mimOE. Is the runtime running?")\nexcept APIError as e:\n    print(f"API error: {e.message}")\n'})})})]}),"\n",(0,i.jsx)(n.h2,{id:"performance-tips",children:"Performance Tips"}),"\n",(0,i.jsx)(n.h3,{id:"first-request-latency",children:"First Request Latency"}),"\n",(0,i.jsx)(n.p,{children:"The first inference request loads the model into memory. This can take 5-30 seconds depending on model size. Subsequent requests are much faster."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution:"})," Pre-load models on startup:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/models" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{"model": "smollm2-360m"}\'\n'})}),"\n",(0,i.jsx)(n.h3,{id:"context-size",children:"Context Size"}),"\n",(0,i.jsx)(n.p,{children:"Stay within the model's context window. If you exceed it, responses may be cut off or the request may fail."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"function trimConversation(messages, maxTokens = 2048) {\n  // Simple estimation: ~4 chars per token\n  let totalChars = 0;\n  const trimmed = [];\n\n  for (let i = messages.length - 1; i >= 0; i--) {\n    const msgChars = messages[i].content.length;\n    if (totalChars + msgChars > maxTokens * 4) break;\n    trimmed.unshift(messages[i]);\n    totalChars += msgChars;\n  }\n\n  return trimmed;\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,i.jsx)(n.p,{children:"Unload models you're not using to free memory:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X DELETE "http://localhost:8083/mimik-ai/openai/v1/models/old-model" \\\n  -H "Authorization: Bearer 1234"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"slow-first-request",children:"Slow First Request"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptom:"})," First inference takes 30+ seconds"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cause:"})," Model loading into memory"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution:"})," This is expected. Pre-load models to avoid user-facing latency."]}),"\n",(0,i.jsx)(n.h3,{id:"out-of-memory",children:"Out of Memory"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptom:"})," Error about insufficient memory"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Use a smaller quantized model (Q4 instead of Q8)"}),"\n",(0,i.jsx)(n.li,{children:"Unload other models from cache"}),"\n",(0,i.jsx)(n.li,{children:"Close other memory-intensive applications"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"token-limit-exceeded",children:"Token Limit Exceeded"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptom:"})," Response is cut off or error about context length"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cause:"})," Every model has a maximum context size. The total tokens (input + output) cannot exceed this limit."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Solution:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Trim conversation history to reduce input tokens"}),"\n",(0,i.jsxs)(n.li,{children:["Adjust ",(0,i.jsx)(n.code,{children:"max_tokens"})," in the chat completion request to limit output tokens"]}),"\n",(0,i.jsxs)(n.li,{children:["Increase ",(0,i.jsx)(n.code,{children:"initContextSize"})," when provisioning the model (see ",(0,i.jsx)(n.a,{href:"/mimOE-devportal/docs/api/model-registry",children:"Model Registry API"}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/mimOE-devportal/docs/ai-foundation/examples/chat-smollm2",children:"Chat with SmolLM2"})}),":  Build a complete chat application"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/mimOE-devportal/docs/ai-foundation/examples/semantic-search",children:"Semantic Search"})}),":  Use embeddings for search"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/mimOE-devportal/docs/api/model-registry",children:"Model Registry API"})}),":  Manage your models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"/mimOE-devportal/docs/api/inference",children:"Inference API Reference"})}),":  Complete API specification"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>l});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}},9365(e,n,t){t.d(n,{A:()=>o});t(6540);var s=t(4164);const i="tabItem_Ymn6";var r=t(4848);function o({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,s.A)(i,t),hidden:n,children:e})}}}]);