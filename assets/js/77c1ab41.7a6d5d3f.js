"use strict";(globalThis.webpackChunkmimoe_devportal=globalThis.webpackChunkmimoe_devportal||[]).push([[6337],{1470(e,n,s){s.d(n,{A:()=>k});var t=s(6540),r=s(4164),i=s(7559),d=s(3104),l=s(6347),o=s(205),c=s(7485),a=s(1682),h=s(679);function x(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function j(e){const{values:n,children:s}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return x(e).map(({props:{value:e,label:n,attributes:s,default:t}})=>({value:e,label:n,attributes:s,default:t}))}(s);return function(e){const n=(0,a.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,s])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function p({queryString:e=!1,groupId:n}){const s=(0,l.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(r),(0,t.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(s.location.search);n.set(r,e),s.replace({...s.location,search:n.toString()})},[r,s])]}function u(e){const{defaultValue:n,queryString:s=!1,groupId:r}=e,i=j(e),[d,l]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=n.find(e=>e.default)??n[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:i})),[c,a]=p({queryString:s,groupId:r}),[x,u]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,r]=(0,h.Dv)(n);return[s,(0,t.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),g=(()=>{const e=c??x;return m({value:e,tabValues:i})?e:null})();(0,o.A)(()=>{g&&l(g)},[g]);return{selectedValue:d,selectValue:(0,t.useCallback)(e=>{if(!m({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),a(e),u(e)},[a,u,i]),tabValues:i}}var g=s(2303);const b="tabList__CuJ",f="tabItem_LNqP";var v=s(4848);function y({className:e,block:n,selectedValue:s,selectValue:t,tabValues:i}){const l=[],{blockElementScrollPositionUntilNextRender:o}=(0,d.a_)(),c=e=>{const n=e.currentTarget,r=l.indexOf(n),d=i[r].value;d!==s&&(o(n),t(d))},a=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const s=l.indexOf(e.currentTarget)+1;n=l[s]??l[0];break}case"ArrowLeft":{const s=l.indexOf(e.currentTarget)-1;n=l[s]??l[l.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:i.map(({value:e,label:n,attributes:t})=>(0,v.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{l.push(e)},onKeyDown:a,onClick:c,...t,className:(0,r.A)("tabs__item",f,t?.className,{"tabs__item--active":s===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:s}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===s);return e?(0,t.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==s}))})}function I(e){const n=u(e);return(0,v.jsxs)("div",{className:(0,r.A)(i.G.tabs.container,"tabs-container",b),children:[(0,v.jsx)(y,{...n,...e}),(0,v.jsx)(_,{...n,...e})]})}function k(e){const n=(0,g.A)();return(0,v.jsx)(I,{...e,children:x(e.children)},String(n))}},7574(e,n,s){s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>j,frontMatter:()=>o,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"api/inference","title":"Inference API","description":"OpenAI-compatible inference API reference for chat, embeddings, and model management","source":"@site/docs/api/inference.md","sourceDirName":"api","slug":"/api/inference","permalink":"/mimOE-devportal/docs/api/inference","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Inference API","description":"OpenAI-compatible inference API reference for chat, embeddings, and model management"},"sidebar":"apiSidebar","next":{"title":"Model Registry API","permalink":"/mimOE-devportal/docs/api/model-registry"}}');var r=s(4848),i=s(8453),d=s(1470),l=s(9365);const o={sidebar_position:3,title:"Inference API",description:"OpenAI-compatible inference API reference for chat, embeddings, and model management"},c="Inference API",a={},h=[{value:"Base URL",id:"base-url",level:2},{value:"Authentication",id:"authentication",level:2},{value:"Quick Reference",id:"quick-reference",level:2},{value:"Chat Completions",id:"chat-completions",level:2},{value:"Create Chat Completion",id:"create-chat-completion",level:3},{value:"Streaming Responses",id:"streaming-responses",level:3},{value:"Multi-Turn Conversations",id:"multi-turn-conversations",level:3},{value:"Tool Calls",id:"tool-calls",level:3},{value:"Generation Parameters",id:"generation-parameters",level:3},{value:"Embeddings",id:"embeddings",level:2},{value:"Create Embeddings",id:"create-embeddings",level:3},{value:"Model Cache Management",id:"model-cache-management",level:2},{value:"List Loaded Models",id:"list-loaded-models",level:3},{value:"Load Model",id:"load-model",level:3},{value:"Unload Model",id:"unload-model",level:3},{value:"Model Lifecycle",id:"model-lifecycle",level:2},{value:"Error Responses",id:"error-responses",level:2},{value:"Common Errors",id:"common-errors",level:3},{value:"OpenAI SDK Compatibility",id:"openai-sdk-compatibility",level:2},{value:"Related",id:"related",level:2}];function x(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"inference-api",children:"Inference API"})}),"\n",(0,r.jsx)(n.p,{children:"The Inference API (mILM) provides OpenAI-compatible inference capabilities for the AI Foundation Package. This service supports chat completions, text embeddings, and model cache management for GGUF models stored in the Model Registry."}),"\n",(0,r.jsx)(n.h2,{id:"base-url",children:"Base URL"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"http://localhost:8083/mimik-ai/openai/v1\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"The Inference API follows the OpenAI API format, making it compatible with existing OpenAI client libraries and tools."})}),"\n",(0,r.jsx)(n.h2,{id:"authentication",children:"Authentication"}),"\n",(0,r.jsx)(n.p,{children:"All endpoints require Bearer token authentication:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Authorization: Bearer <token>\n"})}),"\n",(0,r.jsx)(n.h2,{id:"quick-reference",children:"Quick Reference"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Method"}),(0,r.jsx)(n.th,{children:"Endpoint"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"POST"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"#create-chat-completion",children:(0,r.jsx)(n.code,{children:"/chat/completions"})})}),(0,r.jsx)(n.td,{children:"Generate chat response"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"POST"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"#create-embeddings",children:(0,r.jsx)(n.code,{children:"/embeddings"})})}),(0,r.jsx)(n.td,{children:"Generate text embeddings"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GET"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"#list-loaded-models",children:(0,r.jsx)(n.code,{children:"/models"})})}),(0,r.jsx)(n.td,{children:"List loaded models"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"POST"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"#load-model",children:(0,r.jsx)(n.code,{children:"/models"})})}),(0,r.jsx)(n.td,{children:"Load model into cache"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DELETE"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"#unload-model",children:(0,r.jsx)(n.code,{children:"/models?modelId={id}"})})}),(0,r.jsx)(n.td,{children:"Unload model from cache"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"chat-completions",children:"Chat Completions"}),"\n",(0,r.jsx)(n.p,{children:"Generate chat responses using LLM or VLM models."}),"\n",(0,r.jsx)(n.h3,{id:"create-chat-completion",children:"Create Chat Completion"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"POST /chat/completions\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Headers"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Header"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Content-Type"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"application/json"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Authorization"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Bearer <token>"})})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request Body"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"model"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Model ID from the Model Registry"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"messages"})}),(0,r.jsx)(n.td,{children:"array"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Conversation messages"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"stream"})}),(0,r.jsx)(n.td,{children:"boolean"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Enable streaming responses (default: false)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"temperature"})}),(0,r.jsx)(n.td,{children:"number"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Sampling temperature 0.0-2.0 (default: 1.0)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"top_p"})}),(0,r.jsx)(n.td,{children:"number"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Nucleus sampling threshold 0.0-1.0 (default: 1.0)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"max_tokens"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Maximum tokens to generate"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Message Object"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"role"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsxs)(n.td,{children:["Message role: ",(0,r.jsx)(n.code,{children:"system"}),", ",(0,r.jsx)(n.code,{children:"user"}),", ",(0,r.jsx)(n.code,{children:"assistant"}),", ",(0,r.jsx)(n.code,{children:"tool"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"content"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Message content"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Basic Chat"})}),"\n","\n",(0,r.jsxs)(d.A,{children:[(0,r.jsx)(l.A,{value:"curl",label:"cURL",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "system", "content": "You are a helpful assistant."},\n      {"role": "user", "content": "Complete this sentence: AI is like a"}\n    ]\n  }\'\n'})})}),(0,r.jsx)(l.A,{value:"js",label:"JavaScript",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const response = await fetch('http://localhost:8083/mimik-ai/openai/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer 1234'\n  },\n  body: JSON.stringify({\n    model: 'smollm2-360m',\n    messages: [\n      { role: 'system', content: 'You are a helpful assistant.' },\n      { role: 'user', content: 'Complete this sentence: AI is like a' }\n    ]\n  })\n});\n\nconst result = await response.json();\nconsole.log(result.choices[0].message.content);\n"})})}),(0,r.jsx)(l.A,{value:"python",label:"Python",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\n\nresponse = requests.post(\n    "http://localhost:8083/mimik-ai/openai/v1/chat/completions",\n    headers={\n        "Content-Type": "application/json",\n        "Authorization": "Bearer 1234"\n    },\n    json={\n        "model": "smollm2-360m",\n        "messages": [\n            {"role": "system", "content": "You are a helpful assistant."},\n            {"role": "user", "content": "Complete this sentence: AI is like a"}\n        ]\n    }\n)\n\nresult = response.json()\nprint(result["choices"][0]["message"]["content"])\n'})})}),(0,r.jsx)(l.A,{value:"openai",label:"OpenAI SDK",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8083/mimik-ai/openai/v1",\n    api_key="1234"\n)\n\nresponse = client.chat.completions.create(\n    model="smollm2-360m",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Complete this sentence: AI is like a"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n'})})})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Response (200 OK)"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-abc123",\n  "object": "chat.completion",\n  "created": 1729591200,\n  "model": "smollm2-360m",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Running AI locally offers privacy (your data stays on-device), lower latency (no network round-trips), offline capability, and reduced cloud costs."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 20,\n    "completion_tokens": 32,\n    "total_tokens": 52\n  }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"streaming-responses",children:"Streaming Responses"}),"\n",(0,r.jsx)(n.p,{children:"Enable real-time token streaming for better UX:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [{"role": "user", "content": "Complete this sentence: AI is like a"}],\n    "stream": true\n  }\'\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Streaming Response (SSE)"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1729591200,"model":"smollm2-360m","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}\n\ndata: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1729591200,"model":"smollm2-360m","choices":[{"index":0,"delta":{"content":"Silicon"},"finish_reason":null}]}\n\ndata: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1729591200,"model":"smollm2-360m","choices":[{"index":0,"delta":{"content":" minds"},"finish_reason":null}]}\n\ndata: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1729591200,"model":"smollm2-360m","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}\n\ndata: [DONE]\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"JavaScript Streaming Example"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const response = await fetch('http://localhost:8083/mimik-ai/openai/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer 1234'\n  },\n  body: JSON.stringify({\n    model: 'smollm2-360m',\n    messages: [{ role: 'user', content: 'Complete this sentence: AI is like a' }],\n    stream: true\n  })\n});\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n\n  const chunk = decoder.decode(value);\n  const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n  for (const line of lines) {\n    const data = line.slice(6);\n    if (data === '[DONE]') break;\n\n    const parsed = JSON.parse(data);\n    const content = parsed.choices[0]?.delta?.content;\n    if (content) {\n      process.stdout.write(content);\n    }\n  }\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"multi-turn-conversations",children:"Multi-Turn Conversations"}),"\n",(0,r.jsx)(n.p,{children:"Include conversation history in the messages array:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "user", "content": "What is the capital of France?"},\n      {"role": "assistant", "content": "The capital of France is Paris."},\n      {"role": "user", "content": "What is its population?"}\n    ]\n  }\'\n'})}),"\n",(0,r.jsx)(n.admonition,{title:"Conversation Management",type:"tip",children:(0,r.jsx)(n.p,{children:"The API is stateless. Include the full conversation history in each request. The model doesn't remember previous requests."})}),"\n",(0,r.jsx)(n.h3,{id:"tool-calls",children:"Tool Calls"}),"\n",(0,r.jsxs)(n.p,{children:["mILM parses ",(0,r.jsx)(n.code,{children:"<tool_call>"})," tags from model output and returns structured tool calls:"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Output with Tool Call"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'I\'ll check the weather for you.\n<tool_call>\n{"name": "get_weather", "arguments": {"location": "San Francisco"}}\n</tool_call>\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parsed Response"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "choices": [\n    {\n      "message": {\n        "role": "assistant",\n        "content": "I\'ll check the weather for you.",\n        "tool_calls": [\n          {\n            "id": "call_abc123",\n            "type": "function",\n            "function": {\n              "name": "get_weather",\n              "arguments": "{\\"location\\": \\"San Francisco\\"}"\n            }\n          }\n        ]\n      },\n      "finish_reason": "tool_calls"\n    }\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"generation-parameters",children:"Generation Parameters"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Range"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"temperature"})}),(0,r.jsx)(n.td,{children:"number"}),(0,r.jsx)(n.td,{children:"0.0-2.0"}),(0,r.jsx)(n.td,{children:"1.0"}),(0,r.jsx)(n.td,{children:"Randomness (higher = more creative)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"top_p"})}),(0,r.jsx)(n.td,{children:"number"}),(0,r.jsx)(n.td,{children:"0.0-1.0"}),(0,r.jsx)(n.td,{children:"1.0"}),(0,r.jsx)(n.td,{children:"Nucleus sampling threshold"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"max_tokens"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"1-\u221e"}),(0,r.jsx)(n.td,{children:"model limit"}),(0,r.jsx)(n.td,{children:"Maximum response tokens"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example with Parameters"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [{"role": "user", "content": "Tell me a creative story"}],\n    "temperature": 0.8,\n    "top_p": 0.9,\n    "max_tokens": 500\n  }\'\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"embeddings",children:"Embeddings"}),"\n",(0,r.jsx)(n.p,{children:"Generate vector embeddings from text using embedding models."}),"\n",(0,r.jsx)(n.h3,{id:"create-embeddings",children:"Create Embeddings"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"POST /embeddings\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Headers"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Header"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Content-Type"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"application/json"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Authorization"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Bearer <token>"})})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request Body"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"model"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Embedding model ID"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"input"})}),(0,r.jsx)(n.td,{children:"string or array"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Text(s) to embed"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Single Input"})}),"\n",(0,r.jsxs)(d.A,{children:[(0,r.jsx)(l.A,{value:"curl",label:"cURL",default:!0,children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/embeddings" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "nomic-embed-text",\n    "input": "The quick brown fox jumps over the lazy dog."\n  }\'\n'})})}),(0,r.jsx)(l.A,{value:"js",label:"JavaScript",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"const response = await fetch('http://localhost:8083/mimik-ai/openai/v1/embeddings', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer 1234'\n  },\n  body: JSON.stringify({\n    model: 'nomic-embed-text',\n    input: 'The quick brown fox jumps over the lazy dog.'\n  })\n});\n\nconst result = await response.json();\nconsole.log('Embedding dimensions:', result.data[0].embedding.length);\n"})})}),(0,r.jsx)(l.A,{value:"python",label:"Python",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\n\nresponse = requests.post(\n    "http://localhost:8083/mimik-ai/openai/v1/embeddings",\n    headers={\n        "Content-Type": "application/json",\n        "Authorization": "Bearer 1234"\n    },\n    json={\n        "model": "nomic-embed-text",\n        "input": "The quick brown fox jumps over the lazy dog."\n    }\n)\n\nresult = response.json()\nprint(f"Embedding dimensions: {len(result[\'data\'][0][\'embedding\'])}")\n'})})})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Response (200 OK)"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [\n    {\n      "object": "embedding",\n      "index": 0,\n      "embedding": [0.0023, -0.0094, 0.0152, ...]\n    }\n  ],\n  "model": "nomic-embed-text",\n  "usage": {\n    "prompt_tokens": 10,\n    "total_tokens": 10\n  }\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Batch Input"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/embeddings" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "nomic-embed-text",\n    "input": [\n      "First text to embed",\n      "Second text to embed",\n      "Third text to embed"\n    ]\n  }\'\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Batch Response"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [\n    {"object": "embedding", "index": 0, "embedding": [...]},\n    {"object": "embedding", "index": 1, "embedding": [...]},\n    {"object": "embedding", "index": 2, "embedding": [...]}\n  ],\n  "model": "nomic-embed-text",\n  "usage": {\n    "prompt_tokens": 15,\n    "total_tokens": 15\n  }\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Batch Constraints"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Constraint"}),(0,r.jsx)(n.th,{children:"Limit"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Maximum items"}),(0,r.jsx)(n.td,{children:"50"}),(0,r.jsx)(n.td,{children:"Maximum number of input strings per request"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Input type"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"string"})," or ",(0,r.jsx)(n.code,{children:"string[]"})]}),(0,r.jsx)(n.td,{children:"Each element must be a string"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Token limit"}),(0,r.jsx)(n.td,{children:"Model-specific"}),(0,r.jsx)(n.td,{children:"Each input string is subject to the model's maximum token limit"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Batch Error Codes"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Code"}),(0,r.jsx)(n.th,{children:"Cause"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"400"}),(0,r.jsx)(n.td,{children:"Input array exceeds 50 items"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"400"}),(0,r.jsx)(n.td,{children:"Input array contains non-string elements"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"model-cache-management",children:"Model Cache Management"}),"\n",(0,r.jsx)(n.p,{children:"mILM maintains a runtime cache of loaded models. Models are loaded from the Model Registry on demand."}),"\n",(0,r.jsx)(n.h3,{id:"list-loaded-models",children:"List Loaded Models"}),"\n",(0,r.jsx)(n.p,{children:"List models currently loaded in the runtime cache."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"GET /models\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl "http://localhost:8083/mimik-ai/openai/v1/models" \\\n  -H "Authorization: Bearer 1234"\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Response (200 OK)"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "data": [\n    {\n      "id": "smollm2-360m",\n      "object": "model",\n      "created": 1769534258,\n      "owned_by": "mimik",\n      "info": {\n        "kind": "llm",\n        "chat_template_hint": "chatml",\n        "n_gpu_layers": 99,\n        "max_context": 2048,\n        "n_vocab": 49152,\n        "n_ctx_train": 8192,\n        "n_embd": 960,\n        "n_params": 361821120,\n        "model_size": 384618240\n      },\n      "metrics": {\n        "inference_count": 12,\n        "last_used": 1769534258,\n        "loaded_at": 1769530800,\n        "tokens_per_second": 227.43,\n        "avg_tokens_per_second": 198.65\n      }\n    }\n  ],\n  "object": "list"\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Info Fields"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"kind"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsxs)(n.td,{children:["Model type: ",(0,r.jsx)(n.code,{children:'"llm"'}),", ",(0,r.jsx)(n.code,{children:'"vlm"'}),", or ",(0,r.jsx)(n.code,{children:'"embed"'})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"chat_template_hint"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Chat template applied during loading"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n_gpu_layers"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"Number of layers offloaded to GPU"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"max_context"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"Maximum context size used at load time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n_vocab"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"Vocabulary size"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n_ctx_train"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"Training context length"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n_embd"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"Embedding dimension size"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"n_params"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"Total parameter count"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"model_size"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"Model file size in bytes"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Metrics by Kind"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metric"}),(0,r.jsx)(n.th,{children:"LLM / VLM"}),(0,r.jsx)(n.th,{children:"Embed"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"inference_count"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Total number of inference calls"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"tokens_per_second"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Token throughput of the most recent inference"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"avg_tokens_per_second"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Average token throughput across all inferences"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"last_latency_ms"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Latency of the most recent inference in milliseconds"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"avg_latency_ms"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Average latency across all inferences in milliseconds"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"last_used"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Unix epoch timestamp (seconds) of last inference"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"loaded_at"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Unix epoch timestamp (seconds) when the model was loaded"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example: Embed Model Response"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "data": [\n    {\n      "id": "nomic-embed-text-v1.5.Q8_0",\n      "object": "model",\n      "created": 1769534320,\n      "owned_by": "mimik",\n      "info": {\n        "kind": "embed",\n        "chat_template_hint": "",\n        "n_gpu_layers": -1,\n        "max_context": 2048,\n        "n_vocab": 30522,\n        "n_ctx_train": 2048,\n        "n_embd": 768,\n        "n_params": 136727040,\n        "model_size": 145389792\n      },\n      "metrics": {\n        "inference_count": 1,\n        "last_used": 1769534324,\n        "loaded_at": 1769534320,\n        "last_latency_ms": 10.61,\n        "avg_latency_ms": 10.61\n      }\n    }\n  ],\n  "object": "list"\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"load-model",children:"Load Model"}),"\n",(0,r.jsx)(n.p,{children:"Load a model from the Model Registry into the runtime cache."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"POST /models\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request Body"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"model"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Model ID from the Model Registry"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"chatTemplateHint"})}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsxs)(n.td,{children:["Override the chat template (e.g., ",(0,r.jsx)(n.code,{children:'"chatml"'}),", ",(0,r.jsx)(n.code,{children:'"llama3"'}),", ",(0,r.jsx)(n.code,{children:'"gemma"'}),")"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"initParams"})}),(0,r.jsx)(n.td,{children:"object"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Model initialization overrides"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"initParams.contextSize"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Override the default context window size"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"initParams.gpuLayerSize"})}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Number of layers to offload to GPU"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/models" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m"\n  }\'\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Response (201 Created)"})}),"\n",(0,r.jsxs)(n.p,{children:["The response is streamed as Server-Sent Events (SSE) with ",(0,r.jsx)(n.code,{children:"Content-Type: text/event-stream"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Progress events"})," are emitted while the model loads:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'data: {"progress":"<|loading_model|> 0%"}\n\ndata: {"progress":"<|loading_model|> 25%"}\n\ndata: {"progress":"<|loading_model|> 50%"}\n\ndata: {"progress":"<|loading_model|> 100%"}\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Final event"})," contains the loaded model object:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "id": "smollm2-360m",\n  "object": "model",\n  "created": 1769534258,\n  "owned_by": "mimik",\n  "info": {\n    "kind": "llm",\n    "chat_template_hint": "chatml",\n    "n_gpu_layers": 99,\n    "max_context": 2048,\n    "n_vocab": 49152,\n    "n_ctx_train": 8192,\n    "n_embd": 960,\n    "n_params": 361821120,\n    "model_size": 384618240\n  },\n  "metrics": {\n    "inference_count": 0,\n    "tokens_per_second": 0,\n    "avg_tokens_per_second": 0,\n    "last_used": null,\n    "loaded_at": 1769534258\n  }\n}\n'})}),"\n",(0,r.jsxs)(n.admonition,{title:"Auto-Load",type:"info",children:[(0,r.jsx)(n.p,{children:"Models are automatically loaded on first inference request. Explicit loading is optional but useful for warming up the cache."}),(0,r.jsx)(n.p,{children:"Auto-load can fail with the following errors:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"404"}),": The model ID was not found in the Model Registry."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"400"}),": The model exists but is not ready (",(0,r.jsx)(n.code,{children:"readyToUse: false"}),"). Complete the model provisioning by uploading or downloading the model file first."]}),"\n"]})]}),"\n",(0,r.jsx)(n.h3,{id:"unload-model",children:"Unload Model"}),"\n",(0,r.jsx)(n.p,{children:"Remove a model from the runtime cache. The model files remain in the Model Registry and can be reloaded later."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"DELETE /models?modelId={id}\n"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Location"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"modelId"})}),(0,r.jsx)(n.td,{children:"query"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Model ID to unload"})]})})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X DELETE "http://localhost:8083/mimik-ai/openai/v1/models?modelId=smollm2-360m" \\\n  -H "Authorization: Bearer 1234"\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Response (200 OK)"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "id": "smollm2-360m",\n  "object": "model",\n  "deleted": true\n}\n'})}),"\n",(0,r.jsx)(n.admonition,{title:"Memory Management",type:"tip",children:(0,r.jsx)(n.p,{children:"Unload models you're not actively using to free memory for other models. The model files remain in the Model Registry and can be reloaded later."})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"model-lifecycle",children:"Model Lifecycle"}),"\n",(0,r.jsx)(n.p,{children:"The following diagram shows the relationship between the Model Registry and the Inference API:"}),"\n",(0,r.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Client\n    participant Store as Model Registry\n    participant ILM as Inference API\n\n    Note over Store: Step 1: Provision Model\n    Client->>Store: POST /models (metadata)\n    Client->>Store: POST /models/{id}/download\n\n    Note over ILM: Step 2: Load Model\n    Client->>ILM: POST /models (optional)\n    ILM->>Store: GET /models/{id} (metadata)\n    ILM->>ILM: Load model into cache\n\n    Note over ILM: Step 3: Run Inference\n    Client->>ILM: POST /chat/completions\n    ILM--\x3e>Client: Response\n\n    Note over ILM: Step 4: Unload (optional)\n    Client->>ILM: DELETE /models?modelId={id}"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"error-responses",children:"Error Responses"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Code"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"400"}),(0,r.jsx)(n.td,{children:"Bad request (invalid input or model not ready)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"401"}),(0,r.jsx)(n.td,{children:"Unauthorized (missing or invalid API key)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"404"}),(0,r.jsx)(n.td,{children:"Not found (model not in Model Registry)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"500"}),(0,r.jsx)(n.td,{children:"Internal server error"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Error Format"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "message": "Model \'unknown-model\' not found in store",\n  "statusCode": 404\n}\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["This error format deviates from the standard OpenAI API error format, which wraps errors in an ",(0,r.jsx)(n.code,{children:"error"})," object. Handle errors by checking for ",(0,r.jsx)(n.code,{children:"message"})," and ",(0,r.jsx)(n.code,{children:"statusCode"})," at the top level of the response body."]})}),"\n",(0,r.jsx)(n.h3,{id:"common-errors",children:"Common Errors"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Not Found"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "message": "Model \'smollm2-360m\' not found in store",\n  "statusCode": 404\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"Cause: Model doesn't exist in the Model Registry.\nSolution: Create the model in the Model Registry first."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Not Ready"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "message": "Model \'smollm2-360m\' is not ready (readyToUse: false)",\n  "statusCode": 400\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"Cause: Model metadata exists but file hasn't been uploaded/downloaded.\nSolution: Complete the model provisioning by uploading or downloading the file."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"openai-sdk-compatibility",children:"OpenAI SDK Compatibility"}),"\n",(0,r.jsx)(n.p,{children:"The Inference API is compatible with the official OpenAI Python and JavaScript SDKs:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Python"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8083/mimik-ai/openai/v1",\n    api_key="1234"  # Your mimOE API key\n)\n\n# Chat completion\nresponse = client.chat.completions.create(\n    model="smollm2-360m",\n    messages=[{"role": "user", "content": "Hello!"}]\n)\n\n# Embeddings\nembeddings = client.embeddings.create(\n    model="nomic-embed-text",\n    input="Hello, world!"\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"JavaScript/TypeScript"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import OpenAI from 'openai';\n\nconst client = new OpenAI({\n  baseURL: 'http://localhost:8083/mimik-ai/openai/v1',\n  apiKey: '1234'  // Your mimOE API key\n});\n\n// Chat completion\nconst response = await client.chat.completions.create({\n  model: 'smollm2-360m',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\n// Embeddings\nconst embeddings = await client.embeddings.create({\n  model: 'nomic-embed-text',\n  input: 'Hello, world!'\n});\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"related",children:"Related"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/mimOE-devportal/docs/api/model-registry",children:"Model Registry API"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/mimOE-devportal/docs/ai-foundation/quick-start",children:"AI Foundation Quick Start"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../ai-foundation/inference/",children:"Inference API Guide"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/mimOE-devportal/docs/ai-foundation/examples/chat-smollm2",children:"Chat Example"})}),"\n"]})]})}function j(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(x,{...e})}):x(e)}},8453(e,n,s){s.d(n,{R:()=>d,x:()=>l});var t=s(6540);const r={},i=t.createContext(r);function d(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),t.createElement(i.Provider,{value:n},e.children)}},9365(e,n,s){s.d(n,{A:()=>d});s(6540);var t=s(4164);const r="tabItem_Ymn6";var i=s(4848);function d({children:e,hidden:n,className:s}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,t.A)(r,s),hidden:n,children:e})}}}]);