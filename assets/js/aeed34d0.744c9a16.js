"use strict";(globalThis.webpackChunkmimoe_devportal=globalThis.webpackChunkmimoe_devportal||[]).push([[8515],{8149(e,n,s){s.r(n),s.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ai-foundation/examples/chat-smollm2","title":"Chat with SmolLM2","description":"Build a conversational AI application using SmolLM2 GGUF model","source":"@site/docs/ai-foundation/examples/chat-smollm2.md","sourceDirName":"ai-foundation/examples","slug":"/ai-foundation/examples/chat-smollm2","permalink":"/webdemo/docs/ai-foundation/examples/chat-smollm2","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chat with SmolLM2","description":"Build a conversational AI application using SmolLM2 GGUF model"},"sidebar":"aiFoundationSidebar","previous":{"title":"Examples Overview","permalink":"/webdemo/docs/ai-foundation/examples/"},"next":{"title":"Semantic Search with Embeddings","permalink":"/webdemo/docs/ai-foundation/examples/semantic-search"}}');var o=s(4848),i=s(8453);const a={sidebar_position:2,title:"Chat with SmolLM2",description:"Build a conversational AI application using SmolLM2 GGUF model"},l="Chat with SmolLM2",r={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Provision SmolLM2",id:"step-1-provision-smollm2",level:2},{value:"Step 2: Basic Chat",id:"step-2-basic-chat",level:2},{value:"Step 3: Build a Chat Interface",id:"step-3-build-a-chat-interface",level:2},{value:"JavaScript/Node.js",id:"javascriptnodejs",level:3},{value:"Python",id:"python",level:3},{value:"Step 4: Advanced Features",id:"step-4-advanced-features",level:2},{value:"Adjusting Response Style",id:"adjusting-response-style",level:3},{value:"Context Management",id:"context-management",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Slow First Response",id:"slow-first-response",level:3},{value:"Out of Memory",id:"out-of-memory",level:3},{value:"Poor Response Quality",id:"poor-response-quality",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chat-with-smollm2",children:"Chat with SmolLM2"})}),"\n",(0,o.jsx)(n.p,{children:"Build a conversational AI application using the SmolLM2-360M model. This example walks through setting up the model and creating a complete chat interface."}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This example demonstrates:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Provisioning a GGUF model with two-step provisioning"}),"\n",(0,o.jsx)(n.li,{children:"Making chat completion requests"}),"\n",(0,o.jsx)(n.li,{children:"Handling streaming responses"}),"\n",(0,o.jsx)(n.li,{children:"Building a multi-turn conversation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["mimOE AI Foundation Package running (",(0,o.jsx)(n.a,{href:"/webdemo/docs/ai-foundation/quick-start",children:"Quick Start"}),")"]}),"\n",(0,o.jsx)(n.li,{children:"Node.js 18+ or Python 3.8+ (for code examples)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-1-provision-smollm2",children:"Step 1: Provision SmolLM2"}),"\n",(0,o.jsx)(n.p,{children:"First, create the model metadata:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/store/v1/models" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "id": "smollm2-360m",\n    "version": "1.0.0",\n    "kind": "llm"\n  }\'\n'})}),"\n",(0,o.jsx)(n.p,{children:"Then download the model:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/store/v1/models/smollm2-360m/download" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "url": "https://huggingface.co/lmstudio-community/SmolLM2-360M-Instruct-GGUF/resolve/main/SmolLM2-360M-Instruct-Q8_0.gguf?download=true"\n  }\'\n'})}),"\n",(0,o.jsx)(n.p,{children:"Verify the model is ready:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl "http://localhost:8083/mimik-ai/store/v1/models/smollm2-360m"\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Confirm ",(0,o.jsx)(n.code,{children:"readyToUse: true"})," before proceeding."]}),"\n",(0,o.jsx)(n.h2,{id:"step-2-basic-chat",children:"Step 2: Basic Chat"}),"\n",(0,o.jsx)(n.p,{children:"Send a simple chat request:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "user", "content": "Hello! Can you introduce yourself?"}\n    ]\n  }\'\n'})}),"\n",(0,o.jsx)(n.h2,{id:"step-3-build-a-chat-interface",children:"Step 3: Build a Chat Interface"}),"\n",(0,o.jsx)(n.h3,{id:"javascriptnodejs",children:"JavaScript/Node.js"}),"\n",(0,o.jsx)(n.p,{children:"Install the OpenAI SDK:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"npm install openai readline\n"})}),"\n",(0,o.jsx)(n.p,{children:"Create a chat application:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",metastring:'title="chat.js"',children:"import OpenAI from 'openai';\nimport readline from 'readline';\n\nconst client = new OpenAI({\n  baseURL: 'http://localhost:8083/mimik-ai/openai/v1',\n  apiKey: '1234'\n});\n\nconst messages = [\n  { role: 'system', content: 'You are a helpful AI assistant.' }\n];\n\nconst rl = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout\n});\n\nconsole.log('Chat with SmolLM2 (type \"exit\" to quit)\\n');\n\nasync function chat() {\n  rl.question('You: ', async (input) => {\n    if (input.toLowerCase() === 'exit') {\n      console.log('Goodbye!');\n      rl.close();\n      return;\n    }\n\n    messages.push({ role: 'user', content: input });\n\n    process.stdout.write('SmolLM2: ');\n\n    const stream = await client.chat.completions.create({\n      model: 'smollm2-360m',\n      messages: messages,\n      stream: true\n    });\n\n    let fullResponse = '';\n    for await (const chunk of stream) {\n      const content = chunk.choices[0]?.delta?.content;\n      if (content) {\n        process.stdout.write(content);\n        fullResponse += content;\n      }\n    }\n\n    console.log('\\n');\n    messages.push({ role: 'assistant', content: fullResponse });\n    chat();\n  });\n}\n\nchat();\n"})}),"\n",(0,o.jsx)(n.p,{children:"Run with:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"node chat.js\n"})}),"\n",(0,o.jsx)(n.h3,{id:"python",children:"Python"}),"\n",(0,o.jsx)(n.p,{children:"Install the OpenAI SDK:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,o.jsx)(n.p,{children:"Create a chat application:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="chat.py"',children:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="http://localhost:8083/mimik-ai/openai/v1",\n    api_key="1234"\n)\n\nmessages = [\n    {"role": "system", "content": "You are a helpful AI assistant."}\n]\n\nprint("Chat with SmolLM2 (type \'exit\' to quit)\\n")\n\nwhile True:\n    user_input = input("You: ")\n    if user_input.lower() == "exit":\n        print("Goodbye!")\n        break\n\n    messages.append({"role": "user", "content": user_input})\n\n    print("SmolLM2: ", end="", flush=True)\n\n    stream = client.chat.completions.create(\n        model="smollm2-360m",\n        messages=messages,\n        stream=True\n    )\n\n    full_response = ""\n    for chunk in stream:\n        content = chunk.choices[0].delta.content\n        if content:\n            print(content, end="", flush=True)\n            full_response += content\n\n    print("\\n")\n    messages.append({"role": "assistant", "content": full_response})\n'})}),"\n",(0,o.jsx)(n.p,{children:"Run with:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python chat.py\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-4-advanced-features",children:"Step 4: Advanced Features"}),"\n",(0,o.jsx)(n.h3,{id:"adjusting-response-style",children:"Adjusting Response Style"}),"\n",(0,o.jsx)(n.p,{children:"Use temperature and system prompts to control output:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Creative responses\ncurl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "system", "content": "You are a creative storyteller."},\n      {"role": "user", "content": "Write a short story about a robot."}\n    ],\n    "temperature": 0.9,\n    "max_tokens": 500\n  }\'\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Factual responses\ncurl -X POST "http://localhost:8083/mimik-ai/openai/v1/chat/completions" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{\n    "model": "smollm2-360m",\n    "messages": [\n      {"role": "system", "content": "You are a technical assistant. Be precise and accurate."},\n      {"role": "user", "content": "Explain how HTTPS works."}\n    ],\n    "temperature": 0.2\n  }\'\n'})}),"\n",(0,o.jsx)(n.h3,{id:"context-management",children:"Context Management"}),"\n",(0,o.jsx)(n.p,{children:"Trim conversation history to stay within context limits:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"function trimMessages(messages, maxTokens = 1500) {\n  const systemMessage = messages.find(m => m.role === 'system');\n  const nonSystemMessages = messages.filter(m => m.role !== 'system');\n\n  // Estimate tokens (rough: 4 chars per token)\n  let totalChars = systemMessage ? systemMessage.content.length : 0;\n  const trimmed = [];\n\n  // Keep most recent messages\n  for (let i = nonSystemMessages.length - 1; i >= 0; i--) {\n    const msgChars = nonSystemMessages[i].content.length;\n    if (totalChars + msgChars > maxTokens * 4) break;\n    trimmed.unshift(nonSystemMessages[i]);\n    totalChars += msgChars;\n  }\n\n  return systemMessage ? [systemMessage, ...trimmed] : trimmed;\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(n.h3,{id:"slow-first-response",children:"Slow First Response"}),"\n",(0,o.jsx)(n.p,{children:"The first request loads the model into memory. Pre-load it:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl -X POST "http://localhost:8083/mimik-ai/openai/v1/models" \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer 1234" \\\n  -d \'{"model": "smollm2-360m"}\'\n'})}),"\n",(0,o.jsx)(n.h3,{id:"out-of-memory",children:"Out of Memory"}),"\n",(0,o.jsx)(n.p,{children:"SmolLM2-360M requires only ~1GB RAM. If you still have issues:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Close other memory-intensive applications"}),"\n",(0,o.jsxs)(n.li,{children:["Reduce ",(0,o.jsx)(n.code,{children:"initContextSize"})," when creating the model"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"poor-response-quality",children:"Poor Response Quality"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Verify ",(0,o.jsx)(n.code,{children:'chatTemplateHint: "chatml"'})," matches the model"]}),"\n",(0,o.jsx)(n.li,{children:"Add a clear system prompt"}),"\n",(0,o.jsx)(n.li,{children:"Adjust temperature (lower for factual, higher for creative)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"/webdemo/docs/ai-foundation/examples/semantic-search",children:"Semantic Search"})}),":  Use embeddings for similarity search"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"/webdemo/docs/api/inference",children:"Inference API Reference"})}),":  Complete API documentation"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>a,x:()=>l});var t=s(6540);const o={},i=t.createContext(o);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);